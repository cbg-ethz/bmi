{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#benchmarking-mutual-information","title":"Benchmarking Mutual Information","text":"<p>BMI is the package for estimation of mutual information between continuous random variables and testing new estimators.</p> <ul> <li>Documentation: https://cbg-ethz.github.io/bmi/</li> <li>Source code: https://github.com/cbg-ethz/bmi</li> <li>Bug reports: https://github.com/cbg-ethz/bmi/issues</li> <li>PyPI package: https://pypi.org/project/benchmark-mi</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<p>While we recommend taking a look at the documentation to learn about full package capabilities, below we present the main capabilities of the Python package. (Note that BMI can also be used to test non-Python mutual information estimators.)</p> <p>You can install the package using:</p> <pre><code>$ pip install benchmark-mi\n</code></pre> <p>Alternatively, you can use the development version from source using:</p> <pre><code>$ pip install \"bmi @ https://github.com/cbg-ethz/bmi\"\n</code></pre> <p>Note: BMI uses JAX and by default installs the CPU version of it. If you have a device supporting CUDA, you can install the CUDA version of JAX.</p> <p>Now let's take one of the predefined distributions included in the benchmark (named \"tasks\") and sample 1,000 data points. Then, we will run two estimators on this task.</p> <pre><code>import bmi\n\ntask = bmi.benchmark.BENCHMARK_TASKS['1v1-normal-0.75']\nprint(f\"Task {task.name} with dimensions {task.dim_x} and {task.dim_y}\")\nprint(f\"Ground truth mutual information: {task.mutual_information:.2f}\")\n\nX, Y = task.sample(1000, seed=42)\n\ncca = bmi.estimators.CCAMutualInformationEstimator()\nprint(f\"Estimate by CCA: {cca.estimate(X, Y):.2f}\")\n\nksg = bmi.estimators.KSGEnsembleFirstEstimator(neighborhoods=(5,))\nprint(f\"Estimate by KSG: {ksg.estimate(X, Y):.2f}\")\n</code></pre>"},{"location":"#evaluating-a-new-estimator","title":"Evaluating a new estimator","text":"<p>The above code snippet may be convenient for estimating mutual information on a given data set or for the development of a new mutual information estimator. However, for extensive benchmarking it may be more convenient to use one of the benchmark suites available in the <code>workflows/benchmark/</code> subdirectory.</p> <p>For example, you can install Snakemake and run a small benchmark suite on several estimators using:</p> <pre><code>$ snakemake -c4 -s workflows/benchmark/demo/run.smk\n</code></pre> <p>In about a minute it should generate minibenchmark results in the <code>generated/benchmark/demo</code> directory. Note that the configuration file, <code>workflows/benchmark/demo/config.py</code>, explicitly defines the estimators and tasks used, as well as the number of samples.</p> <p>Hence, it is easy to benchmark a custom estimator by importing it and including it in the configuration dictionary. More information is available here, where we cover evaluating new Python as well as non-Python estimators.</p> <p>Similarly, it is easy to change the number of samples or adjust the tasks included in the benchmark. We defined several benchmark suites with shared structure.</p>"},{"location":"#list-of-implemented-estimators","title":"List of implemented estimators","text":"<p>(Your estimator can be here too! Please, reach out to us if you would like to contribute.)</p> <ul> <li>The neighborhood-based KSG estimator proposed in Estimating Mutual Information by Kraskov et al. (2003).</li> <li>Donsker-Varadhan and MINE estimators proposed in MINE: Mutual Information Neural Estimation by Belghazi et al. (2018).</li> <li>InfoNCE estimator proposed in Representation Learning with Contrastive Predictive Coding by Oord et al. (2018).</li> <li>NWJ estimator proposed in Estimating divergence functionals and the likelihood ratio by convex risk minimization by Nguyen et al. (2008).</li> <li>Estimator based on canonical correlation analysis described in Feature discovery under contextual supervision using mutual information by Kay (1992) and in Some data analyses using mutual information by Brillinger (2004).</li> </ul>"},{"location":"#references","title":"References","text":""},{"location":"#new-on-the-properties-and-estimation-of-pointwise-mutual-information-profiles","title":"\u2728 New! \u2728 On the properties and estimation of pointwise mutual information profiles","text":"<p>In this manuscript we discuss the pointwise mutual information profile, an invariant which can be used to diagnose limitations of the previous mutual information benchmark, and a flexible distribution family of Bend and Mix Models. These distributions can be used to create more expressive benchmark tasks and provide model-based Bayesian estimates of mutual information.</p> <p>Workflows:   - To run the updated version of the benchmark, using Bend and Mix Models, see <code>workflows/benchmark/v2</code>.   - To reproduce the experimental results from the manuscript, see <code>workflows/projects/Mixtures</code>.</p> <pre><code>@article{\n  pmi-profiles-2025,\n  title={On the Properties and Estimation of Pointwise Mutual Information Profiles},\n  author={Czy{\\.z}, Pawe{\\l} and Grabowski, Frederic and Vogt, Julia and Beerenwinkel, Niko and Marx, Alexander},\n  journal={Transactions on Machine Learning Research},\n  issn={2835-8856},\n  year={2025},\n  url={https://openreview.net/forum?id=LdflD41Gn8},\n  note={}\n}\n</code></pre>"},{"location":"#beyond-normal-on-the-evaluation-of-the-mutual-information-estimators","title":"Beyond normal: On the evaluation of the mutual information estimators","text":"<p>In this manuscript we discuss a benchmark for mutual information estimators.</p> <p>Workflows:   - To run the benchmark, see <code>workflows/benchmark/v1</code>.   - To reproduce the experimental results from the manuscript, see <code>workflows/projects/Beyond_Normal</code>.</p> <pre><code>@inproceedings{beyond-normal-2023,\n title = {Beyond Normal: On the Evaluation of Mutual Information Estimators},\n author = {Czy\\.{z}, Pawe{\\l}  and Grabowski, Frederic and Vogt, Julia and Beerenwinkel, Niko and Marx, Alexander},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},\n pages = {16957--16990},\n publisher = {Curran Associates, Inc.},\n url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/36b80eae70ff629d667f210e13497edf-Paper-Conference.pdf},\n volume = {36},\n year = {2023}\n}\n</code></pre>"},{"location":"benchmarking-new-estimator/","title":"Benchmarking a new estimator","text":"<p>Standard package utilities allow one to estimate mutual information on a given sample and can be convenient for the development of a new mutual information estimator. However, once a new estimator is ready, how can it be benchmarked? Running it manually on many distributions can take a lot of effort.</p> <p>Conveniently, Snakemake is a workflow orchestrator which can generate benchmark tasks, save samples to the disk, and run many estimators. Once Snakemake has been installed, we recommend running:</p> <pre><code>$ snakemake -c4 -s workflows/benchmark/demo/run.smk\n</code></pre> <p>In about a minute it should generate minibenchmark results in the <code>generated/benchmark/demo</code> directory. Note that the configuration file, <code>workflows/benchmark/demo/config.py</code>, explicitly defines the estimators and tasks used, as well as the number of samples.</p> <p>Hence, it is possible to benchmark a custom Python estimator by simply importing it and including it in the configuration dictionary. Similarly, it is easy to change the number of samples or used tasks. The <code>config.py</code> file is plain Python!</p> <p>We defined several benchmark suites with the shared structure.</p>"},{"location":"benchmarking-new-estimator/#adding-a-new-python-estimator","title":"Adding a new Python estimator","text":"<p>Every added estimator has to implement the <code>IMutualInformationPointEstimator</code> interface.</p> <p>Let's take a look at the simplest possible estimator (which generally shouldn't be used in practice), which estimates mutual information as \\(\\hat I(X; Y) = -0.5 \\log\\left( 1-\\mathrm{Corr}(X_1, Y_1)^2 \\right)\\).</p> <pre><code>import numpy as np\n\nfrom bmi.interface import BaseModel, IMutualInformationPointEstimator\n\nclass EmptyParams(BaseModel):\n    \"\"\"This estimator has no hyperparameters.\"\"\"\n    pass\n\nclass TooSimpleEstimator(IMutualInformationPointEstimator):\n    def __init__(self) -&gt; None:\n        \"\"\"All estimator hyperparameters should be set at this stage.\n        This estimator does not have any hyperparameters, though.\n        \"\"\"\n        pass\n\n    def estimate(self, x, y) -&gt; float:\n        \"\"\"Estimates mutual information.\n\n        Attrs:\n            x: samples from the X variable, shape (n_points, dim_x)\n            y: samples from the Y variable, shape (n_points, dim_y)\n        \"\"\"\n        x, y = np.asarray(x), np.asarray(y)\n        x1 = x[:, 0]\n        y1 = y[:, 0]\n\n        rho = np.corrcoef(x1, y1)[0, 1]\n        return -0.5 * np.log(1 - rho**2)\n\n    def parameters(self) -&gt; BaseModel:\n        \"\"\"Returns the hyperparameters of the estimator.\"\"\"\n        return EmptyParams()\n</code></pre> <p>If your estimator is a function, you can also wrap it into a class using <code>FunctionalEstimator</code> wrapper.</p> <p>Once such a class is available, it can be simply included into the configuration dictionary.</p>"},{"location":"benchmarking-new-estimator/#adding-a-non-python-estimator","title":"Adding a non-Python estimator","text":"<p>How can one use the benchmark to evaluate an estimator implemented in another programming language? The benchmark employs Snakemake workflows in which each estimator has to implement the <code>IMutualInformationPointEstimator</code> interface.</p> <p>However, it's possible to create a thin wrapper, which whenever called will execute the following steps:</p> <ul> <li>Save the samples to a temporary location.</li> <li>Run an external script (e.g., in Julia, R, C++ or any other language) which prints mutual information estimate to the standard output.</li> <li>Convert the standard output to <code>float</code> and return it as the estimate.</li> </ul> <p>The external script can be implemented in any language. Generally it will take three required arguments:</p> <ol> <li>The path to the CSV file with samples.</li> <li>Number of columns representing the \\(X\\) variable, <code>dim_x</code>.</li> <li>Number of columns representing the \\(Y\\) variable, <code>dim_y</code>.</li> </ol> <p>Additionally, it can take any additional arguments controlling the hyperparameters. Example scripts running several existing estimators implemented in Julia and R are here.</p> <p>Once a script is ready and can be run manually on CSV samples, it can be wrapped into a Python class implementing the <code>IMutualInformationPointEstimator</code> interface. Routines such as saving the samples to a CSV file or checking the standard output are standard, so we implemented a convenient class, <code>ExternalEstimator</code>.</p> <p>For example, let's take a look how the wrapper around the KSG estimator in R can be implemented.</p> <pre><code>from pathlib import Path\n\nfrom bmi.estimators.external.external_estimator import ExternalEstimator\nfrom bmi.interface import BaseModel, Pathlike\n\nR_PATH = \"path_to_the_R_script\"\n\n\nclass KSGParams(BaseModel):\n    \"\"\"The hyperparameters of a given estimator.\n\n    Attrs:\n        neighbors (int): Number of neighbors to be used in the KSG algorithm\n    \"\"\"\n    neighbors: int\n\n\nclass RKSGEstimator(ExternalEstimator):\n    \"\"\"The KSG estimators implemented in the `rmi` package in R.\"\"\"\n\n    def __init__(self, neighbors: int = 10) -&gt; None:\n        \"\"\"\n        Args:\n            neighbors: number of neighbors (k) to be used\n        \"\"\"\n        self._params = KSGParams(\n            variant=variant,\n            neighbors=neighbors,\n        )\n\n    def parameters(self) -&gt; KSGParams:\n        \"\"\"Returns the hyperparameters of the estimator.\"\"\"\n        return self._params\n\n    def _build_command(self, path: Pathlike, dim_x: int, dim_y: int) -&gt; list[str]:\n        sample_path_abs = str(Path(path).absolute())\n        return [\n            \"Rscript\",\n            estimator_r_path_abs,\n            sample_path_abs,\n            str(dim_x),\n            str(dim_y),\n            \"--method\",\n            f\"KSG1\",\n            \"--neighbors\",\n            str(self._params.neighbors),\n        ]\n\n\nestimator = RKSGEstimator(neighbors=5)\n# This is an ordinary estimator now and can be run with `estimator.estimate(X, Y)`\n</code></pre> <p>More Python wrappers around external scripts are implemented here.</p>"},{"location":"benchmarking-new-estimator/#faq","title":"FAQ","text":""},{"location":"benchmarking-new-estimator/#how-to-add-an-estimator-implemented-in-another-programming-language","title":"How to add an estimator implemented in another programming language?","text":"<p>It's possible to quickly implement a thin Python wrapper around an estimator implemented in another language. See this section for details.</p>"},{"location":"bmm/","title":"Bend and Mix Models","text":"<p>In this tutorial we will take a closer look at the family of Bend and Mix Models (BMMs), proposed in On the properties and estimation of pointwise mutual information profiles manuscript.</p> <p>A distribution \\(P_{XY}\\) is a BMM, if it is possible to evaluate the densities \\(\\log p_{XY}(x, y)\\), \\(\\log p_X(x)\\), and \\(\\log p_Y(y)\\) for arbitrary points and to efficiently generate samples from \\(P_{XY}\\). In particular, one can evaluate pointwise mutual information </p> <p>\\(\\mathrm{PMI}(x, y) = \\log \\frac{p_{XY}(x, y)}{p_X(x)p_Y(y)}\\)</p> <p>and use a Monte Carlo approximation of the mutual information \\(I(X; Y) = \\mathbb E_{(x, y)\\sim P_{XY}}[\\mathrm{PMI}(x, y)]\\).</p> <p>A BMM can therefore be implemented as a triple of TensorFlow Probability on JAX distributions: the joint distribution \\(P_{XY}\\) and marginals \\(P_X\\) and \\(P_Y\\).</p> <p>For example, let's create a multivariate normal distribution:</p> <pre><code>import jax.numpy as jnp\nfrom bmi.samplers import bmm\n\n# Define a BMM\ncov = jnp.asarray([[1., 0.8], [0.8, 1.]])\ndist = bmm.MultivariateNormalDistribution(dim_x=1, dim_y=1, covariance=cov)\n</code></pre> <p>To see how BMMs can be used in the benchmark, see this section.</p>"},{"location":"bmm/#basic-operations-supported-by-bmms","title":"Basic operations supported by BMMs","text":"<p>A BMM \\(P_{XY}\\) can be used to sample from \\(P_{XY}\\) or evaluate the pointwise mutual information \\(\\mathrm{PMI}(x, y)\\) at any point. The distribution of \\(\\mathrm{PMI}(x, y)\\), where \\((x, y)\\sim P_{XY}\\) is called the PMI profile and can be approximated via the histograms from the samples. Similarly, sample-based Monte Carlo approximation can be used to estimate the mutual information \\(I(X; Y)\\), which is the mean of the PMI profile.</p> <pre><code>import jax.numpy as jnp\nfrom jax import random\n\nfrom bmi.samplers import bmm\n\n# Define a Bend and Mix model:\ncov = jnp.asarray([[1., 0.8], [0.8, 1.]])\ndist = bmm.MultivariateNormalDistribution(dim_x=1, dim_y=1, covariance=cov)\n\n# Sample 100_000 points from the distribution:\nkey = random.PRNGKey(42)\nn_samples = 100_000\nX, Y = dist.sample(1000, key=key)\n\n# Calculate the PMI, obtaining the sample from the PMI profile:\npmis = dist.pmi(X, Y)\n\n# If one wants only the samples from the PMI profile,\n# one can use a helper function.\n# We use the same key to get the same output as in the previous example.\npmis = bmm.pmi_profile(key=key, dist=dist, n=1000)\n\n# Estimate mutual information from samples:\nprint(jnp.mean(pmis))  # 0.51192063\n\n# One can also estimate the mutual information\n# and the associated Monte Carlo Standard Error (MCSE)\n# using the helper function:\nmi_estimate, mi_mcse = bmm.estimate_mutual_information(key=key, dist=dist, n=n_samples)\nprint(mi_estimate)  # 0.51192063\nprint(mi_mcse)  # 0.00252177\n</code></pre>"},{"location":"bmm/#combining-and-transforming-bmms","title":"Combining and transforming BMMs","text":"<p>One can construct new Bend and Mix Models from the existing ones by two basic operations: bending (transforming with a diffeomorphism) or constructing a mixture of given BMMs.</p>"},{"location":"bmm/#transformation-by-a-diffeomorphism","title":"Transformation by a diffeomorphism","text":"<p>A BMM \\(P_{XY}\\) can be bent, i.e. transformed by a diffeomorphism of the form \\(f\\times g\\) to obtain a new BMM \\(P_{f(X)g(Y)}\\). Any diffeomorphism supported by TensorFlow Probability on JAX can be used.</p> <pre><code>import jax.numpy as jnp\nfrom tensorflow_probability.substrates import jax as tfp\n\nfrom bmi.samplers import bmm, SparseLVMParametrization\n\n# Define a normal distribution\ncov = SparseLVMParametrization(dim_x=2, dim_y=3, n_interacting=1).covariance\nnormal = bmm.MultivariateNormalDistribution(dim_x=2, dim_y=3, covariance=cov)\n\n# Use a TensorFlow Probability bijector to transform the distribution\ntransformed_normal = bmm.transform(dist=normal, x_transform=tfp.bijectors.Sigmoid(), y_transform=tfp.bijectors.Sigmoid())\n</code></pre> <p>Note that samplers can be transformed by arbitrary continuous injective functions, not only diffeomorphisms, which preserve mutual information. However, this comes at the cost of losing the ability to compute pointwise mutual information. To wrap a BMM into a sampler, see this section.</p>"},{"location":"bmm/#constructing-a-mixture","title":"Constructing a mixture","text":"<p>If \\(P_{X_1Y_1}, \\dotsc, P_{X_KY_K}\\) are arbitrary BMMs defined on the same space \\(\\mathcal X\\times \\mathcal Y\\), and \\(w_1, \\dotsc, w_K\\) are positive numbers such that \\(w_1 + \\dotsc + w_K=1\\), then the mixture</p> \\[P_{XY} = \\sum_{k=1}^K w_k P_{X_kY_K}\\] <p>is also a BMM. Note that even if the component distributions do not encode any information (\\(I(X_k; Y_k) = 0\\)), the mixture \\(P_{XY}\\) can have \\(I(X; Y) &gt; 0\\) and be as large as \\(\\log K\\).</p> <pre><code>import jax.numpy as jnp\nfrom tensorflow_probability.substrates import jax as tfp\n\nfrom bmi.samplers import bmm, SparseLVMParametrization\n\n# Define a normal distribution\ncov1 = SparseLVMParametrization(dim_x=2, dim_y=3, n_interacting=1).covariance\nnormal1 = bmm.MultivariateNormalDistribution(dim_x=2, dim_y=3, covariance=cov1)\n\ncov2 = SparseLVMParametrization(dim_x=2, dim_y=3, n_interacting=2).covariance\nnormal2 = bmm.MultivariateNormalDistribution(dim_x=2, dim_y=3, covariance=cov2)\n\nmixture = bmm.mixture(proportions=[0.8, 0.2], components=[normal1, normal2])\n</code></pre>"},{"location":"bmm/#discrete-variables","title":"Discrete variables","text":"<p>When both variables are discrete (and over an alphabet which is not too large), the joint distribution \\(P_{XY}\\) can be represented by a probability table, and the mutual information can be computed exactly and the <code>dit</code> package is an excellent choice for these applications.</p> <p>However, discrete distributions \\(P_{XY}\\) are also BMMs and a Monte Carlo approximation can be used. Moreover, the BMMs are flexible enough to model some cases where one of the variables is discrete and the other is continuous.</p> <p>Consider a family of distributions \\(P_{X_z}\\otimes P_{Y_z}\\), where each variable \\(X_z\\) is a continuous variable and \\(Y_z\\) is a discrete variable. Although we have \\(I(X_z; Y_z) = 0\\), by mixing the distributions \\(P_{X_z}\\otimes P_{Y_z}\\) with different \\(z\\) we can obtain a distribution with non-zero mutual information, represented by a Bayesian network \\(X \\leftarrow Z\\rightarrow Y\\).</p> <p>Below we will show an example: </p> <pre><code>import jax.numpy as jnp\n\nfrom bmi.samplers import bmm\n\nfrom tensorflow_probability.substrates import jax as tfp\ntfd = tfp.distributions\n\ndef construct_bernoulli(p: float, dtype=jnp.float64) -&gt; tfd.Distribution:\n    \"\"\"Constructs a Bernoulli distribution, as\n    TensorFlow Probability disallows products of continuous\n    and discrete distributions.\"\"\"\n    return tfd.Independent(\n            tfd.Bernoulli(probs=jnp.asarray([p], dtype=dtype), dtype=dtype),\n            reinterpreted_batch_ndims=1,\n    )\n\n\n# Define the distributions X_k. Each of them is a continuous distribution on R^2.\nx1 = bmm.construct_multivariate_student_distribution(mean=-jnp.ones(2), dispersion=0.2 * jnp.eye(2), df=8)\nx2 = bmmm.construct_multivariate_normal_distribution(mean=jnp.zeros(2), covariance=jnp.asarray([[1.0, 0.8], [0.8, 1.0]]))\nx3 = bmm.construct_multivariate_student_distribution(mean=jnp.ones(2), dispersion=0.2 * jnp.eye(2), df=5)\n\n# Define the distributions for Y_k. Each of them is a discrete distribution over the alphabet {0, 1}.\ny1 = construct_bernoulli(0.95)\ny2 = construct_bernoulli(0.5)\ny3 = construct_bernoulli(0.05)\n\n# Define the product distributions P(X_k, Y_k).\ncomponents = [bmm.ProductDistribution(dist_x, dist_y) for dist_x, dist_y in zip([x1, x2, x3], [y1, y2, y3])]\n\n# Construct the distribution P(X, Y) distribution.\n# As this is a BMM, one can construct the PMI profile, approximate MI, etc.\njoint_distribution = bmm.mixture(proportions=[0.25, 0.5, 0.25], components=components)\n</code></pre>"},{"location":"bmm/#connecting-bmms-and-samplers","title":"Connecting BMMs and samplers","text":"<p>One of the main abstractions of the proposed package is the <code>ISampler</code> class, which holds a joint probability distribution with known mutual information. As such, samplers can be transformed with arbitrary continuous injective functions, combined with other samplers or used to define named benchmark tasks.</p> <p>Fine distributions can be used to create samplers using the <code>BMMSampler</code> wrapper. For example, let's create a sampler for a bivariate normal distribution:</p> <pre><code>import jax.numpy as jnp\nfrom bmi.samplers import bmm\n\n# Define a BMM\ncov = jnp.asarray([[1.0, 0.8], [0.8, 1.0]])\ndist = bmm.MultivariateNormalDistribution(dim_x=1, dim_y=1, covariance=cov)\n\n# Wrap the distribution into a sampler.\n# We will use 10 000 samples to estimate ground-truth mutual information.\n# Generally, the more samples, the better.\nsampler = bmm.FineSampler(dist, mi_estimate_sample=10_000, mi_estimate_seed=42)\n\nprint(sampler.mutual_information())  # 0.5178049\n# Created sampler can be used as usual (transformed, used to create tasks, combined, etc.)\n</code></pre>"},{"location":"bmm/#estimating-mutual-information-with-bmms","title":"Estimating mutual information with BMMs","text":"<p>Consider a parametric family of BMMs, \\(\\{P_\\theta \\mid \\theta \\in \\mathcal T\\}\\), where \\(P_{\\theta}\\) is a model for a joint distribution \\(P(X, Y\\mid \\theta)\\). For example, if \\(X\\) and \\(Y\\) are assumed to be jointly multivariate norrmal, each \\(P_\\theta\\) will be a multivariate norrmal distribution with \\(\\theta\\) representing the mean vector and covariance matrix. As each \\(P_\\theta\\) is a BMM, one can calculate the mutual information between the idealised variables \\(X\\) and \\(Y\\) for different parameters \\(\\theta\\).</p> <p>Hence, once a data set \\((x_1, y_1), \\dotsc, (x_N, y_N)\\) is available, one can construct the Bayesian posterior on the parameters \\(P(\\theta \\mid (x_1, y_1),\\dotsc, (x_N, y_N))\\) and use it to obtain the posterior distribution on \\(I(X; Y)\\).</p> <p>Hence, the workflow for model-based estimation of mutual information is as follows:</p> <ol> <li> <p>Construct a parametric family of distributions in a probabilistic programming language (e.g.,  TensorFlow Probability on JAX or NumPyro).</p> </li> <li> <p>Ensure that for each parameter \\(\\theta\\), you can construct the distribution \\(P_\\theta\\) (e.g., by implementing a simple wrapper).</p> </li> <li>Use the data set to construct the posterior distribution \\(P(\\theta \\mid (x_1, y_1),\\dotsc, (x_N, y_N))\\).</li> <li>Ensure that the model is well-specified and does not underfit or overfit the data.</li> <li>Wrap the samples from the posterior in BMMs and use Monte Carlo approximation to estimate the mutual information to obtain the samples from the mutual information posterior.</li> </ol> <p>As an example using NumPyro to fit a mixture of multivariate normal distributions, see <code>workflows/Mixtures/fitting_gmm.smk</code>.</p>"},{"location":"bmm/#faq","title":"FAQ","text":""},{"location":"bmm/#where-is-the-api","title":"Where is the API?","text":"<p>The API is here.</p>"},{"location":"bmm/#what-is-the-difference-between-a-bmm-and-a-sampler","title":"What is the difference between a BMM and a sampler?","text":"<p>Samplers are more general than BMMs. This section explains how to wrap a given BMM into a sampler.</p>"},{"location":"bmm/#how-should-i-choose-the-number-of-samples-for-the-mutual-information-estimation","title":"How should I choose the number of samples for the mutual information estimation?","text":"<p>If variance of the PMI profile is finite and equal to \\(V\\), then the Monte Carlo Standard Error (MCSE) of the mutual information estimate is equal to \\(\\sqrt{V/n}\\). We suggest to obtain an estimate for e.g., 10,000 samples (which should be fast) and observing whether the MCSE is small enough. Additionally, we suggest to repeat sampling several times and observing whether the MCSE is stable: it may be possible that for some problems the variance of the PMI profile may be infinite and MCSE is not a valid uncertainty estimate.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your time to contribute to this project! Below we present some guidelines.</p>"},{"location":"contributing/#reporting-a-bug","title":"Reporting a bug","text":"<p>If you find a bug, please submit a new issue.</p> <p>To be able to reproduce a bug, we will usually need the following information:</p> <ul> <li>Versions of Python packages used (in particular version of this library).</li> <li>A minimal code snippet allowing us to reproduce the bug.</li> <li>What is the desired behaviour in the reported case?</li> <li>What is the actual behaviour?</li> </ul>"},{"location":"contributing/#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>Do:</p> <ul> <li>Do use Google Style Guide. We use black for code formatting.</li> <li>Do write unit tests. We use pytest.</li> <li>Do write docstrings. We use Material for Mkdocs to generate the documentation.</li> <li>Do write high-level documentation as examples and tutorials, illustrating introduced features.</li> <li>Do consider submitting a draft pull request with a description of proposed changes.</li> <li>Do check the Development section.</li> </ul> <p>Don't:</p> <ul> <li>Don't include license information. This project uses the MIT license. By submitting your pull request you implicitly and irrevocably agree to using this license.</li> <li>Don't implement too many ideas in a single pull request. Multiple features should be implemented in separate pull requests.</li> </ul>"},{"location":"contributing/#development","title":"Development","text":""},{"location":"contributing/#installation-dependencies","title":"Installation &amp; dependencies","text":"<p>To install the repository together with the dependencies run: <pre><code>$ git clone git@github.com:cbg-ethz/bmi.git  # Clone the repository\n$ poetry add install -G dev                  # Install the dependencies\n$ poetry run pre-commit install              # Install pre-commit hooks\n$ poetry run pytest                          # Check if unit tests are passing\n</code></pre></p>"},{"location":"contributing/#building-documentation-locally","title":"Building documentation locally","text":"<p>You can build the documentation on your machine using: <pre><code>$ poetry run mkdocs serve\n</code></pre> and opening the generated link using web browser.</p>"},{"location":"estimators/","title":"Estimators","text":"<p>The package supports a range of existing mutual information estimators. For the full list, see below.</p>"},{"location":"estimators/#example","title":"Example","text":"<p>The design of the estimators was motivated by SciKit-Learn API<sup>1</sup>. All estimators are classes. Once a class is initialized, one can use the <code>estimate</code> method, which maps arrays containing data points (of shape <code>(n_points, n_dim)</code>)  to mutual information estimates:</p> <pre><code>import bmi\n\n# Generate a sample with 1000 data points\ntask = bmi.benchmark.BENCHMARK_TASKS['1v1-normal-0.75']\nX, Y = task.sample(1000, seed=42)\nprint(f\"X shape: {X.shape}\")  # Shape (1000, 1)\nprint(f\"Y shape: {Y.shape}\")  # Shape (1000, 1)\n\n# Once an estimator is instantiated, it can be used to estimate mutual information\n# by using the `estimate` method.\ncca = bmi.estimators.CCAMutualInformationEstimator()\nprint(f\"Estimate by CCA: {cca.estimate(X, Y):.2f}\")\n\nksg = bmi.estimators.KSGEnsembleFirstEstimator(neighborhoods=(5,))\nprint(f\"Estimate by KSG: {ksg.estimate(X, Y):.2f}\")\n</code></pre> <p>Additionally, the estimators can be queried for their hyperparameters: <pre><code>print(cca.parameters())  # CCA does not have tunable hyperparameters\n# _EmptyParams()\n\nprint(ksg.parameters())  # KSG has tunable hyperparameters\n# KSGEnsembleParameters(neighborhoods=[5], standardize=True, metric_x='euclidean', metric_y='euclidean')\n</code></pre></p> <p>The returned objects are structured using Pydantic.</p>"},{"location":"estimators/#list-of-estimators","title":"List of estimators","text":""},{"location":"estimators/#neural-estimators","title":"Neural estimators","text":"<p>We support several standard neural estimators in JAX basing on the PyTorch implementations<sup>2</sup>: </p> <ul> <li>Donsker-Varadhan estimator<sup>3</sup> is implemented in <code>DonskerVaradhanEstimator</code>.</li> <li>MINE<sup>3</sup> estimator, which is a Donsker-Varadhan estimator with correction debiasing gradient during the fitting phase, is implemented in <code>MINEEstimator</code>.</li> <li>InfoNCE<sup>4</sup>, also known as Contrastive Predictive Coding, is implemented in <code>InfoNCEEstimator</code>.</li> <li>NWJ estimator<sup>5</sup> is implemented as <code>NWJEstimator</code>.</li> </ul>"},{"location":"estimators/#model-based-estimators","title":"Model-based estimators","text":"<ul> <li>Canonical correlation analysis<sup>6</sup><sup>7</sup> is suitable when \\(P(X, Y)\\) is multivariate normal and does not require hyperparameter tuning. It's implemented in <code>CCAMutualInformationEstimator</code>.</li> </ul>"},{"location":"estimators/#histogram-based-estimators","title":"Histogram-based estimators","text":"<ul> <li>We implement a histogram-based estimator<sup>8</sup> in <code>HistogramEstimator</code>. However, note that we do not support adaptive binning schemes.</li> </ul>"},{"location":"estimators/#kernel-density-estimators","title":"Kernel density estimators","text":"<ul> <li>We implement a simple kernel density estimator in <code>KDEMutualInformationEstimator</code>.</li> </ul>"},{"location":"estimators/#neighborhood-based-estimators","title":"Neighborhood-based estimators","text":"<ul> <li>An ensemble of Kraskov-St\u00f6gbauer-Grassberger estimators<sup>9</sup> is implemented as <code>KSGEnsembleFirstEstimator</code>.</li> </ul>"},{"location":"estimators/#faq","title":"FAQ","text":""},{"location":"estimators/#do-these-estimators-work-for-discrete-variables","title":"Do these estimators work for discrete variables?","text":"<p>When both variables \\(X\\) and \\(Y\\) are discrete, we recommend the <code>dit</code> package. When one variable is discrete and the other is continuous, one can approximate mutual information by adding small noise to the discrete variable. </p> <p>Todo</p> <p>Add a Python example showing how to add the noise.</p>"},{"location":"estimators/#where-is-the-api-showing-how-to-use-the-estimators","title":"Where is the API showing how to use the estimators?","text":"<p>The API is here.</p>"},{"location":"estimators/#how-can-i-add-a-new-estimator","title":"How can I add a new estimator?","text":"<p>Thank you for considering contributing to this project! Please, consult contributing guidelines and reach out to us on GitHub, so we can discuss the best way of adding the estimator to the package.</p> <p>Generally, the following steps are required:</p> <ol> <li>Implement the interface <code>IMutualInformationPointEstimator</code> in a new file inside <code>src/bmi/estimators</code> directory. The unit tests should be added in <code>tests/estimators</code> directory.</li> <li>Export the new estimator to the public API by adding an entry in <code>src/bmi/estimators/__init__.py</code>.</li> <li>Export the docstring of new estimator to <code>docs/api/estimators.md</code>.</li> <li>Add the estimator to the list of estimators and ReadMe</li> </ol> <ol> <li> <p>Lars Buitinck and others. API design for machine learning software: experiences from the scikit-learn project. arXiv, 9 2013. arXiv:1309.0238.\u00a0\u21a9</p> </li> <li> <p>Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information estimators. CoRR, 2019. URL: http://arxiv.org/abs/1910.06222, arXiv:1910.06222.\u00a0\u21a9</p> </li> <li> <p>Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International conference on machine learning, 531\u2013540. PMLR, 2018.\u00a0\u21a9\u21a9</p> </li> <li> <p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\u00a0\u21a9</p> </li> <li> <p>XuanLong Nguyen, Martin J Wainwright, and Michael Jordan. Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2007. URL: https://proceedings.neurips.cc/paper_files/paper/2007/file/72da7fd6d1302c0a159f6436d01e9eb0-Paper.pdf.\u00a0\u21a9</p> </li> <li> <p>David R. Brillinger. Some data analyses using mutual information. Brazilian Journal of Probability and Statistics, 18(2):163\u2013182, 2004. URL: http://www.jstor.org/stable/43601047 (visited on 2023-09-24).\u00a0\u21a9</p> </li> <li> <p>J. Kay. Feature discovery under contextual supervision using mutual information. In IJCNN International Joint Conference on Neural Networks, volume 4, 79\u201384. 1992. doi:10.1109/IJCNN.1992.227286.\u00a0\u21a9</p> </li> <li> <p>Christopher J Cellucci, Alfonso M Albano, and Paul E Rapp. Statistical validation of mutual information calculations: comparison of alternative numerical algorithms. Physical review E, 71(6):066208, 2005.\u00a0\u21a9</p> </li> <li> <p>Alexander Kraskov, Harald St\u00f6gbauer, and Peter Grassberger. Estimating mutual information. Physical Review E, 69(6):066138, 2004.\u00a0\u21a9</p> </li> </ol>"},{"location":"api/","title":"API","text":""},{"location":"api/#tasks","title":"Tasks","text":"<p>Tasks represent named probability distributions which are used in the benchmark.</p>"},{"location":"api/#estimators","title":"Estimators","text":"<p>Estimators are the implemented mutual information estimators.</p>"},{"location":"api/#samplers","title":"Samplers","text":"<p>Samplers represent joint probability distributions with known mutual information from which one can sample. They are lower level than <code>Tasks</code> and can be used to define new tasks by transformations which preserve mutual information.</p>"},{"location":"api/#bend-and-mix-models","title":"Bend and Mix Models","text":"<p>Subpackage implementing distributions known as Bend and Mix Models, for which the ground-truth mutual information may not be known analytically, but can be efficiently approximated using Monte Carlo methods. </p>"},{"location":"api/#interfaces","title":"Interfaces","text":"<p>Interfaces defines the main interfaces used in the package.</p>"},{"location":"api/bmm/","title":"Bend and Mix Models","text":""},{"location":"api/bmm/#core-utilities","title":"Core utilities","text":""},{"location":"api/bmm/#bmi.samplers._tfp._core.JointDistribution","title":"<code> bmi.samplers._tfp._core.JointDistribution        </code>  <code>dataclass</code>","text":"<p>The main object of this package, representing a Bend and Mix Model (BMM), i.e., a joint distribution \\(P_{XY}\\) together with the marginal distributions \\(P_X\\) and \\(P_Y\\).</p> <p>Attributes:</p> Name Type Description <code>dist</code> <p>\\(P_{XY}\\)</p> <code>dist_x</code> <code>Distribution</code> <p>\\(P_X\\)</p> <code>dist_y</code> <code>Distribution</code> <p>\\(P_Y\\)</p> <code>dim_x</code> <code>int</code> <p>dimension of the support of \\(X\\)</p> <code>dim_y</code> <code>int</code> <p>dimension of the support of \\(Y\\)</p> <code>analytic_mi</code> <code>Optional[float]</code> <p>analytical mutual information. Use <code>None</code> if unknown (in most cases)</p>"},{"location":"api/bmm/#bmi.samplers._tfp._core.JointDistribution.pmi","title":"<code>pmi(self, x, y)</code>","text":"<p>Calculates pointwise mutual information at specified points.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>points in the X space, shape <code>(n_points, dim_x)</code></p> required <code>y</code> <code>Array</code> <p>points in the Y space, shape <code>(n_points, dim_y)</code></p> required <p>Returns:</p> Type Description <code>Array</code> <p>pointwise mutual information evaluated at (x, y) points,     shape <code>(n_points,)</code></p> <p>Note</p> <p>This function is vectorized, i.e. it can calculate PMI for multiple points at once.</p>"},{"location":"api/bmm/#bmi.samplers._tfp._core.JointDistribution.sample","title":"<code>sample(self, n_points, key)</code>","text":"<p>Sample from the joint distribution \\(P_{XY}\\).</p> <p>Parameters:</p> Name Type Description Default <code>n_points</code> <code>int</code> <p>number of samples to draw</p> required <code>key</code> <code>Array</code> <p>JAX random key</p> required"},{"location":"api/bmm/#bmi.samplers._tfp._core.monte_carlo_mi_estimate","title":"<code>bmi.samplers._tfp._core.monte_carlo_mi_estimate(key, dist, n)</code>","text":"<p>Estimates the mutual information \\(I(X; Y)\\) using Monte Carlo sampling.</p> <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>mutual information estimate standard error estimate</p> <p>Note</p> <p>It is worth to run this procedure multiple times and see whether the standard error estimate is accurate.</p>"},{"location":"api/bmm/#bmi.samplers._tfp._core.pmi_profile","title":"<code>bmi.samplers._tfp._core.pmi_profile(key, dist, n)</code>","text":"<p>Monte Carlo draws a sample of size <code>n</code> from the PMI distribution.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Array</code> <p>JAX random key, used to generate the sample</p> required <code>dist</code> <code>JointDistribution</code> <p>distribution</p> required <code>n</code> <code>int</code> <p>number of points to sample</p> required <p>Returns:</p> Type Description <code>Array</code> <p>PMI profile, shape <code>(n,)</code></p>"},{"location":"api/bmm/#bmi.samplers._tfp._core.transform","title":"<code>bmi.samplers._tfp._core.transform(dist, x_transform=None, y_transform=None)</code>","text":"<p>For given diffeomorphisms \\(f\\) and \\(g\\) transforms the joint distribution \\(P_{XY}\\) into \\(P_{f(X)g(Y)}\\).</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>JointDistribution</code> <p>distribution to be transformed</p> required <code>x_transform</code> <code>Optional[tensorflow_probability.substrates.jax.bijectors.bijector.Bijector]</code> <p>diffeomorphism \\(f\\) to transform \\(X\\). Defaults to identity.</p> <code>None</code> <code>y_transform</code> <code>Optional[tensorflow_probability.substrates.jax.bijectors.bijector.Bijector]</code> <p>diffeomorphism \\(g\\) to transform \\(Y\\). Defaults to identity.</p> <code>None</code> <p>Returns:</p> Type Description <code>JointDistribution</code> <p>transformed distribution</p>"},{"location":"api/bmm/#bmi.samplers._tfp._product.ProductDistribution","title":"<code> bmi.samplers._tfp._product.ProductDistribution            (JointDistribution)         </code>","text":"<p>From distributions \\(P_X\\) and \\(P_Y\\) creates a distribution \\(P_{XY} = P_X \\otimes P_Y\\), so that \\(X\\) and \\(Y\\) are independent.</p> <p>In particular, \\(I(X; Y) = 0\\).</p>"},{"location":"api/bmm/#bmi.samplers._tfp._product.ProductDistribution.__init__","title":"<code>__init__(self, dist_x, dist_y)</code>  <code>special</code>","text":"<p>Creates a product distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dist_x</code> <code>Distribution</code> <p>distribution \\(P_X\\)</p> required <code>dist_y</code> <code>Distribution</code> <p>distribution \\(P_Y\\)</p> required"},{"location":"api/bmm/#bmi.samplers._tfp._wrapper.BMMSampler","title":"<code> bmi.samplers._tfp._wrapper.BMMSampler            (BaseSampler)         </code>","text":"<p>Wraps a given Bend and Mix Model (BMM) into a sampler.</p>"},{"location":"api/bmm/#bmi.samplers._tfp._wrapper.BMMSampler.__init__","title":"<code>__init__(self, dist, mi=None, mi_estimate_seed=0, mi_estimate_sample=200000)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dist</code> <code>JointDistribution</code> <p>distribution represented by a BMM to be wrapped</p> required <code>mi</code> <code>Optional[float]</code> <p>mutual information of the distribution, if already calculated. If not provided, it will be estimated via Monte Carlo sampling.</p> <code>None</code> <code>mi_estimate_seed</code> <code>Union[Any, int]</code> <p>seed for the Monte Carlo sampling</p> <code>0</code> <code>mi_estimate_sample</code> <code>int</code> <p>number of samples for the Monte Carlo sampling</p> <code>200000</code>"},{"location":"api/bmm/#bmi.samplers._tfp._wrapper.BMMSampler.mutual_information","title":"<code>mutual_information(self)</code>","text":"<p>Mutual information MI(X; Y).</p>"},{"location":"api/bmm/#bmi.samplers._tfp._wrapper.BMMSampler.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Returns a sample from the joint distribution P(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>n_points</code> <code>int</code> <p>sample size</p> required <code>rng</code> <code>Union[int, Any]</code> <p>pseudorandom number generator</p> required <p>Returns:</p> Type Description <code>tuple[jax.Array, jax.Array]</code> <p>X samples, shape (n_points, dim_x) Y samples, shape (n_points, dim_y). Note that these samples are paired with X samples.</p>"},{"location":"api/bmm/#basic-distributions","title":"Basic distributions","text":""},{"location":"api/bmm/#bmi.samplers._tfp._normal.construct_multivariate_normal_distribution","title":"<code>bmi.samplers._tfp._normal.construct_multivariate_normal_distribution(mean, covariance)</code>","text":"<p>Constructs a multivariate normal distribution.</p>"},{"location":"api/bmm/#bmi.samplers._tfp._normal.MultivariateNormalDistribution","title":"<code> bmi.samplers._tfp._normal.MultivariateNormalDistribution            (JointDistribution)         </code>","text":"<p>Multivariate normal distribution \\(P_{XY}\\), such that \\(P_X\\) is a multivariate normal distribution on the space of dimension <code>dim_x</code> and \\(P_Y\\) is a multivariate normal distribution on the space of dimension <code>dim_y</code>.</p>"},{"location":"api/bmm/#bmi.samplers._tfp._normal.MultivariateNormalDistribution.__init__","title":"<code>__init__(self, *, dim_x, dim_y, covariance, mean=None)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>dimension of the \\(X\\) support</p> required <code>dim_y</code> <code>int</code> <p>dimension of the \\(Y\\) support</p> required <code>mean</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>mean vector, shape <code>(n,)</code> where <code>n = dim_x + dim_y</code>. Default: zero vector</p> <code>None</code> <code>covariance</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>covariance matrix, shape (n, n)</p> required"},{"location":"api/bmm/#bmi.samplers._tfp._student.construct_multivariate_student_distribution","title":"<code>bmi.samplers._tfp._student.construct_multivariate_student_distribution(mean, dispersion, df)</code>","text":"<p>Constructs a multivariate Student distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Array</code> <p>location vector, shape <code>(dim,)</code></p> required <code>dispersion</code> <code>Array</code> <p>dispersion matrix, shape <code>(dim, dim)</code></p> required <code>df</code> <code>Union[int, float]</code> <p>degrees of freedom</p> required"},{"location":"api/bmm/#bmi.samplers._tfp._student.MultivariateStudentDistribution","title":"<code> bmi.samplers._tfp._student.MultivariateStudentDistribution            (JointDistribution)         </code>","text":"<p>Multivariate Student distribution \\(P_{XY}\\), such that \\(P_X\\) is a multivariate Student distribution on the space of dimension <code>dim_x</code> and \\(P_Y\\) is a multivariate Student distribution on the space of dimension <code>dim_y</code>.</p> <p>Note that the degrees of freedom <code>df</code> are the same for all distributions.</p>"},{"location":"api/bmm/#bmi.samplers._tfp._student.MultivariateStudentDistribution.__init__","title":"<code>__init__(self, *, dim_x, dim_y, df, dispersion, mean=None)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>dimension of the \\(X\\) support</p> required <code>dim_y</code> <code>int</code> <p>dimension of the \\(Y\\) support</p> required <code>df</code> <code>int</code> <p>degrees of freedom</p> required <code>mean</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>mean vector, shape <code>(n,)</code> where <code>n = dim_x + dim_y</code>. Default: zero vector</p> <code>None</code> <code>dispersion</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>dispersion matrix, shape <code>(n, n)</code></p> required"},{"location":"api/estimators/","title":"Estimators","text":"<p>The package implements a number of estimators with common API. For general usage instructions and examples, consult the general instructions. Although the API is organized alphabetically, the estimators have been grouped by types in the list of estimators together with relevant literature.</p>"},{"location":"api/estimators/#bmi.estimators.correlation.CCAMutualInformationEstimator","title":"<code> bmi.estimators.correlation.CCAMutualInformationEstimator            (IMutualInformationPointEstimator)         </code>","text":""},{"location":"api/estimators/#bmi.estimators.correlation.CCAMutualInformationEstimator.__init__","title":"<code>__init__(self, scale=True)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/estimators/#bmi.estimators.correlation.CCAMutualInformationEstimator.estimate","title":"<code>estimate(self, x, y)</code>","text":"<p>A point estimate of MI(X; Y) from an i.i.d. sample from the \\(P(X, Y)\\) distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>shape <code>(n_samples, dim_x)</code></p> required <code>y</code> <p>shape <code>(n_samples, dim_y)</code></p> required <p>Returns:</p> Type Description <p>mutual information estimate</p>"},{"location":"api/estimators/#bmi.estimators.correlation.CCAMutualInformationEstimator.parameters","title":"<code>parameters(self)</code>","text":"<p>Returns the parameters of the estimator.</p>"},{"location":"api/estimators/#bmi.estimators.neural._estimators.DonskerVaradhanEstimator","title":"<code> bmi.estimators.neural._estimators.DonskerVaradhanEstimator            (NeuralEstimatorBase)         </code>","text":""},{"location":"api/estimators/#bmi.estimators._histogram.HistogramEstimator","title":"<code> bmi.estimators._histogram.HistogramEstimator            (IMutualInformationPointEstimator)         </code>","text":""},{"location":"api/estimators/#bmi.estimators._histogram.HistogramEstimator.__init__","title":"<code>__init__(self, n_bins_x=5, n_bins_y=None, standardize=True)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n_bins_x</code> <code>int</code> <p>number of bins per each X dimension</p> <code>5</code> <code>n_bins_y</code> <code>Optional[int]</code> <p>number of bins per each Y dimension. Leave to None to use <code>n_bins_x</code></p> <code>None</code> <code>standardize</code> <code>bool</code> <p>whether to standardize the data set</p> <code>True</code>"},{"location":"api/estimators/#bmi.estimators._histogram.HistogramEstimator.estimate","title":"<code>estimate(self, x, y)</code>","text":"<p>MI estimate.</p>"},{"location":"api/estimators/#bmi.estimators._histogram.HistogramEstimator.parameters","title":"<code>parameters(self)</code>","text":"<p>Returns the parameters of the estimator.</p>"},{"location":"api/estimators/#bmi.estimators.neural._estimators.InfoNCEEstimator","title":"<code> bmi.estimators.neural._estimators.InfoNCEEstimator            (NeuralEstimatorBase)         </code>","text":""},{"location":"api/estimators/#bmi.estimators._kde.KDEMutualInformationEstimator","title":"<code> bmi.estimators._kde.KDEMutualInformationEstimator            (IMutualInformationPointEstimator)         </code>","text":"<p>The kernel density mutual information estimator based on</p> <p>\\(I(X; Y) = h(X) + h(Y) - h(X, Y)\\),</p> <p>where \\(h(X)\\) is the differential entropy \\(h(X) = -\\mathbb{E}[ \\log p(X) ]\\).</p> <p>The logarithm of probability density function \\(\\log p(X)\\) is estimated via a kernel density estimator (KDE) using SciKit-Learn.</p> <p>Note</p> <p>This estimator is very sensitive to the choice of the bandwidth and the kernel. We suggest to treat it with caution.</p>"},{"location":"api/estimators/#bmi.estimators._kde.KDEMutualInformationEstimator.__init__","title":"<code>__init__(self, kernel_xy='tophat', kernel_x=None, kernel_y=None, bandwidth_xy='scott', bandwidth_x=None, bandwidth_y=None, standardize=True)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>kernel_xy</code> <code>Literal['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine']</code> <p>kernel to be used for joint distribution PDF \\(p_{XY}\\) estimation. See SciKit-Learn's <code>KernelDensity</code> object for more information.</p> <code>'tophat'</code> <code>kernel_x</code> <code>Optional[Literal['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine']]</code> <p>kernel to be used for the :math:<code>p_X</code> estimation. If <code>None</code> (default), <code>kernel_xy</code> will be used.</p> <code>None</code> <code>kernel_y</code> <code>Optional[Literal['gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine']]</code> <p>similarly to <code>kernel_x</code>.</p> <code>None</code> <code>bandwidth_xy</code> <code>Union[float, Literal['scott', 'silverman']]</code> <p>kernel bandwidth to be used for joint distribution estimation.</p> <code>'scott'</code> <code>bandwidth_x</code> <code>Union[float, Literal['scott', 'silverman']]</code> <p>kernel bandwidth to be used for \\(p_X\\) estimation. If set to None (default), then <code>bandwidth_xy</code> is used.</p> <code>None</code> <code>bandwidth_y</code> <code>Union[float, Literal['scott', 'silverman']]</code> <p>similar to <code>bandwidth_x</code></p> <code>None</code> <code>standardize</code> <code>bool</code> <p>whether to standardize the data points</p> <code>True</code>"},{"location":"api/estimators/#bmi.estimators._kde.KDEMutualInformationEstimator.estimate","title":"<code>estimate(self, x, y)</code>","text":"<p>A point estimate of MI(X; Y) from an i.i.d. sample from the \\(P(X, Y)\\) distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>shape <code>(n_samples, dim_x)</code></p> required <code>y</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>shape <code>(n_samples, dim_y)</code></p> required <p>Returns:</p> Type Description <code>float</code> <p>mutual information estimate</p>"},{"location":"api/estimators/#bmi.estimators._kde.KDEMutualInformationEstimator.estimate_entropies","title":"<code>estimate_entropies(self, x, y)</code>","text":"<p>Calculates differential entropies.</p> <p>Note</p> <p>Differential entropy is not invariant to standardization. In particular, if you want to estimate differential entropy of the original variables, you should use <code>standardize=False</code>.</p>"},{"location":"api/estimators/#bmi.estimators._kde.KDEMutualInformationEstimator.parameters","title":"<code>parameters(self)</code>","text":"<p>Returns the parameters of the estimator.</p>"},{"location":"api/estimators/#bmi.estimators.ksg.KSGEnsembleFirstEstimator","title":"<code> bmi.estimators.ksg.KSGEnsembleFirstEstimator            (IMutualInformationPointEstimator)         </code>","text":"<p>An implementation of of the neighborhood-based KSG estimator.</p> <p>We use the first approximation (i.e., equation (8) in the paper) and allow for using different neighborhood sizes. The final estimate is the average of the estimates using different neighborhood sizes.</p>"},{"location":"api/estimators/#bmi.estimators.ksg.KSGEnsembleFirstEstimator.__init__","title":"<code>__init__(self, neighborhoods=(5, 10), standardize=True, metric_x='euclidean', metric_y=None, n_jobs=1, chunk_size=10)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>neighborhoods</code> <code>Sequence[int]</code> <p>sequence of positive integers, specifying the size of neighborhood for MI calculation</p> <code>(5, 10)</code> <code>standardize</code> <code>bool</code> <p>whether to standardize the data before MI calculation, by default true</p> <code>True</code> <code>metric_x</code> <code>Literal['euclidean', 'manhattan', 'chebyshev']</code> <p>metric on the X space</p> <code>'euclidean'</code> <code>metric_y</code> <code>Optional[Literal['euclidean', 'manhattan', 'chebyshev']]</code> <p>metric on the Y space. If None, <code>metric_x</code> will be used</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>number of jobs to be launched to compute distances. Use -1 to use all processors.</p> <code>1</code> <code>chunk_size</code> <code>int</code> <p>internal batch size, used to speed up the computations while fitting into the memory</p> <code>10</code> <p>Note</p> <p>If you use Chebyshev (\\(\\l_\\infty\\)) distance for both \\(X\\) and \\(Y\\) spaces, <code>KSGChebyshevEstimator</code> may be faster.</p>"},{"location":"api/estimators/#bmi.estimators.ksg.KSGEnsembleFirstEstimator.estimate","title":"<code>estimate(self, x, y)</code>","text":"<p>A point estimate of MI(X; Y) from an i.i.d. sample from the \\(P(X, Y)\\) distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>shape <code>(n_samples, dim_x)</code></p> required <code>y</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>shape <code>(n_samples, dim_y)</code></p> required <p>Returns:</p> Type Description <code>float</code> <p>mutual information estimate</p>"},{"location":"api/estimators/#bmi.estimators.ksg.KSGEnsembleFirstEstimator.parameters","title":"<code>parameters(self)</code>","text":"<p>Returns the parameters of the estimator.</p>"},{"location":"api/estimators/#bmi.estimators.neural._mine_estimator.MINEEstimator","title":"<code> bmi.estimators.neural._mine_estimator.MINEEstimator            (IMutualInformationPointEstimator)         </code>","text":""},{"location":"api/estimators/#bmi.estimators.neural._mine_estimator.MINEEstimator.trained_critic","title":"<code>trained_critic: Optional[equinox._module.Module]</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the critic function from the end of the training.</p> <p>Note:   1. You need to train the model by estimating mutual information,     otherwise <code>None</code> is returned.   2. Note that the critic can have different meaning depending on     the function used.</p>"},{"location":"api/estimators/#bmi.estimators.neural._mine_estimator.MINEEstimator.__init__","title":"<code>__init__(self, batch_size=256, max_n_steps=10000, train_test_split=0.5, test_every_n_steps=250, learning_rate=0.1, hidden_layers=(16, 8), smoothing_alpha=0.9, standardize=True, verbose=True, seed=42)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/estimators/#bmi.estimators.neural._mine_estimator.MINEEstimator.estimate","title":"<code>estimate(self, x, y)</code>","text":"<p>A point estimate of MI(X; Y) from an i.i.d. sample from the \\(P(X, Y)\\) distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>shape <code>(n_samples, dim_x)</code></p> required <code>y</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>shape <code>(n_samples, dim_y)</code></p> required <p>Returns:</p> Type Description <code>float</code> <p>mutual information estimate</p>"},{"location":"api/estimators/#bmi.estimators.neural._mine_estimator.MINEEstimator.estimate_with_info","title":"<code>estimate_with_info(self, x, y)</code>","text":"<p>Allows for reporting additional information about the run.</p>"},{"location":"api/estimators/#bmi.estimators.neural._mine_estimator.MINEEstimator.parameters","title":"<code>parameters(self)</code>","text":"<p>Returns the parameters of the estimator.</p>"},{"location":"api/estimators/#bmi.estimators.neural._estimators.NWJEstimator","title":"<code> bmi.estimators.neural._estimators.NWJEstimator            (NeuralEstimatorBase)         </code>","text":""},{"location":"api/interfaces/","title":"Interfaces","text":"<p>This section lists the most important interfaces used in the package.</p>"},{"location":"api/interfaces/#bmi.interface.IMutualInformationPointEstimator","title":"<code> bmi.interface.IMutualInformationPointEstimator            (Protocol)         </code>","text":"<p>Interface for the mutual information estimator returning point estimates. All estimators should be implementations of this interface.</p>"},{"location":"api/interfaces/#bmi.interface.IMutualInformationPointEstimator.estimate","title":"<code>estimate(self, x, y)</code>","text":"<p>A point estimate of MI(X; Y) from an i.i.d. sample from the \\(P(X, Y)\\) distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>shape <code>(n_samples, dim_x)</code></p> required <code>y</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>shape <code>(n_samples, dim_y)</code></p> required <p>Returns:</p> Type Description <code>float</code> <p>mutual information estimate</p>"},{"location":"api/interfaces/#bmi.interface.IMutualInformationPointEstimator.estimate_with_info","title":"<code>estimate_with_info(self, x, y)</code>","text":"<p>Allows for reporting additional information about the run.</p>"},{"location":"api/interfaces/#bmi.interface.IMutualInformationPointEstimator.parameters","title":"<code>parameters(self)</code>","text":"<p>Returns the parameters of the estimator.</p>"},{"location":"api/interfaces/#bmi.interface.ISampler","title":"<code> bmi.interface.ISampler            (Protocol)         </code>","text":"<p>Interface for a distribution \\(P(X, Y)\\).</p>"},{"location":"api/interfaces/#bmi.interface.ISampler.dim_total","title":"<code>dim_total: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Dimension of the space in which the <code>(X, Y)</code> variable is valued. Should be equal to the sum of <code>dim_x</code> and <code>dim_y</code>.</p>"},{"location":"api/interfaces/#bmi.interface.ISampler.dim_x","title":"<code>dim_x: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Dimension of the space in which the <code>X</code> variable is valued.</p>"},{"location":"api/interfaces/#bmi.interface.ISampler.dim_y","title":"<code>dim_y: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Dimension of the space in which the <code>Y</code> variable is valued.</p>"},{"location":"api/interfaces/#bmi.interface.ISampler.mutual_information","title":"<code>mutual_information(self)</code>","text":"<p>Mutual information MI(X; Y).</p>"},{"location":"api/interfaces/#bmi.interface.ISampler.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Returns a sample from the joint distribution P(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>n_points</code> <code>int</code> <p>sample size</p> required <code>rng</code> <code>Union[int, Any]</code> <p>pseudorandom number generator</p> required <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, numpy.ndarray]</code> <p>X samples, shape (n_points, dim_x) Y samples, shape (n_points, dim_y). Note that these samples are paired with X samples.</p>"},{"location":"api/samplers/","title":"Samplers","text":"<p>Samplers represent probability distributions with known mutual information.</p>"},{"location":"api/samplers/#simple-distributions","title":"Simple distributions","text":""},{"location":"api/samplers/#bmi.samplers._splitmultinormal.SplitMultinormal","title":"<code> bmi.samplers._splitmultinormal.SplitMultinormal            (BaseSampler)         </code>","text":"<p>Represents two variables with jointly multivariate normal distribution</p> <p>Covariance matrix should have the block form:</p> \\[\\Sigma = \\begin{pmatrix}         \\Sigma_{XX} &amp; \\Sigma_{XY} \\\\         \\Sigma_{YX} &amp; \\Sigma_{YY} \\end{pmatrix}\\] <p>where:</p> <ul> <li>\\(\\Sigma_{XX}\\) is the covariance matrix of \\(X\\) variable (shape <code>(dim_x, dim_x)</code>),</li> <li>\\(\\Sigma_{YY}\\) is the covariance of the \\(Y\\) variable (shape <code>(dim_y, dim_y)</code>)</li> <li>\\(\\Sigma_{XY}\\) and \\(\\Sigma_{YX}\\)   (being transposes of each other, as the matrix is symmetric,   of shapes <code>(dim_x, dim_y)</code> or transposed one) describe the covariance between \\(X\\) and \\(Y\\).</li> </ul>"},{"location":"api/samplers/#bmi.samplers._splitmultinormal.SplitMultinormal.__init__","title":"<code>__init__(self, *, dim_x, dim_y, covariance, mean=None)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>dimension of the X space</p> required <code>dim_y</code> <code>int</code> <p>dimension of the Y space</p> required <code>mean</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>mean vector, shape <code>(n,)</code> where <code>n = dim_x + dim_y</code>. Default: zero vector</p> <code>None</code> <code>covariance</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>covariance matrix, shape (n, n)</p> required"},{"location":"api/samplers/#bmi.samplers._splitmultinormal.SplitMultinormal.mutual_information","title":"<code>mutual_information(self)</code>","text":"<p>Calculates the mutual information I(X; Y) using an exact formula.</p> <p>Returns:</p> Type Description <code>float</code> <p>mutual information, in nats</p> <p>Mutual information is given by</p> \\[I(X; Y) = \\frac 12 \\log \\left(\\frac{\\det(\\Sigma_{XX}) \\det(\\Sigma_{YY})}{\\det(\\Sigma)}\\right)\\] <p>which follows from the formula     \\(I(X; Y) = H(X) + H(Y) - H(X, Y)\\) and the formula for the differential entropy of the multivariate normal distribution.</p>"},{"location":"api/samplers/#bmi.samplers._splitmultinormal.SplitMultinormal.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Returns a sample from the joint distribution P(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>n_points</code> <code>int</code> <p>sample size</p> required <code>rng</code> <code>Union[Any, int]</code> <p>pseudorandom number generator</p> required <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, numpy.ndarray]</code> <p>X samples, shape (n_points, dim_x) Y samples, shape (n_points, dim_y). Note that these samples are paired with X samples.</p>"},{"location":"api/samplers/#bmi.samplers._split_student_t.SplitStudentT","title":"<code> bmi.samplers._split_student_t.SplitStudentT            (BaseSampler)         </code>","text":"<p>Multivariate Student-t distribution.</p> <p>Sampling is based on Wikipedia</p> <p>Mutual information is based on:</p> <p>R.B. Arellano-Valle, J.E. Contreras-Reyes, M.G. Genton, Shannon Entropy and Mutual Information for Multivariate Skew-Elliptical Distributions, Scandinavian Journal of Statistics, vol. 40, pp. 46-47, 2013</p> <p>Note that the final formula for the mutual information is slightly wrong, but can be calculated using the expressions involving differential entropies above.</p>"},{"location":"api/samplers/#bmi.samplers._split_student_t.SplitStudentT.df","title":"<code>df: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Degrees of freedom.</p>"},{"location":"api/samplers/#bmi.samplers._split_student_t.SplitStudentT.__init__","title":"<code>__init__(self, *, dim_x, dim_y, df, dispersion, mean=None)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>dim_x</code> <code>int</code> <p>dimension of the X variable</p> required <code>dim_y</code> <code>int</code> <p>dimension of the Y variable</p> required <code>df</code> <code>int</code> <p>degrees of freedom, strictly positive. Use <code>np.inf</code> for a Gaussian</p> required <code>dispersion</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>dispersion matrix, shape <code>(dim_x + dim_y, dim_x + dim_y)</code></p> required <code>mean</code> <code>Union[numpy.__array_like._SupportsArray[numpy.dtype[Any]], numpy.__nested_sequence._NestedSequence[numpy.__array_like._SupportsArray[numpy.dtype[Any]]], bool, int, float, complex, str, bytes, numpy.__nested_sequence._NestedSequence[Union[bool, int, float, complex, str, bytes]]]</code> <p>mean of the distribution, shape <code>(dim_x + dim_y,)</code>. Default: zero vector</p> <code>None</code> <p>Note</p> <p>Dispersion is not the covariance matrix. To calculate the covariance matrix, use the <code>covariance</code> method.</p>"},{"location":"api/samplers/#bmi.samplers._split_student_t.SplitStudentT.covariance","title":"<code>covariance(self)</code>","text":"<p>Calculates the covariance matrix.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>array, shape <code>(dim_x+dim_y, dim_x+dim_y)</code></p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if covariance is not defined (for <code>df</code> \\(\\le 2\\))</p>"},{"location":"api/samplers/#bmi.samplers._split_student_t.SplitStudentT.mi_correction","title":"<code>mi_correction(self)</code>","text":"<p>Correction term in MI calculation.</p> <p>This term depends only on the dimensionality of each variable and the degrees of freedom. (It does not depend on the dispersion matrix or mean vector used).</p>"},{"location":"api/samplers/#bmi.samplers._split_student_t.SplitStudentT.mi_correction_function","title":"<code>mi_correction_function(df, dim_x, dim_y)</code>  <code>staticmethod</code>","text":"<p>Mutual information between variables with joint Multivariate Student-t decomposes as the sum of mutual information of Gaussian variables and the correction term.</p>"},{"location":"api/samplers/#bmi.samplers._split_student_t.SplitStudentT.mi_normal","title":"<code>mi_normal(self)</code>","text":"<p>Part of the mutual information corresponding to a multivariate normal with covariance given by the dispersion matrix.</p>"},{"location":"api/samplers/#bmi.samplers._split_student_t.SplitStudentT.mutual_information","title":"<code>mutual_information(self)</code>","text":"<p>Expression for MI taken from Arellano-Valle et al., p. 47.</p> <p>This expression consists of two terms:     <code>mi_normal</code>, which is the MI of a multivariate normal distribution       with covariance given by the dispersion     <code>mi_correction</code>, which is a correction term which does not depend       on the means or the dispersion</p>"},{"location":"api/samplers/#bmi.samplers._split_student_t.SplitStudentT.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Sampling from multivariate Student distribution.</p> <p>Note</p> <p>This function is based on SciPy's sampling.</p>"},{"location":"api/samplers/#bmi.samplers._additive_uniform.AdditiveUniformSampler","title":"<code> bmi.samplers._additive_uniform.AdditiveUniformSampler            (BaseSampler)         </code>","text":""},{"location":"api/samplers/#bmi.samplers._additive_uniform.AdditiveUniformSampler.__init__","title":"<code>__init__(self, epsilon)</code>  <code>special</code>","text":"<p>Represents the distribution \\(P_{XY}\\) under the following model:</p> \\[X \\sim \\mathrm{Uniform}(0, 1)\\] \\[N \\sim \\mathrm{Uniform}(-\\epsilon, \\epsilon)\\] \\[Y = X + N\\] <p>The MI in this case is:</p> \\[I(X; Y) = \\begin{cases}      \\frac{1}{4 \\epsilon} \\mathrm{~if~} \\epsilon &gt; 0.5    \\\\      \\epsilon - \\log(2 \\epsilon) \\mathrm{~otherwise}     \\end{cases}\\] <p>and can be derived analytically.</p>"},{"location":"api/samplers/#bmi.samplers._additive_uniform.AdditiveUniformSampler.mutual_information","title":"<code>mutual_information(self)</code>","text":"<p>Mutual information MI(X; Y).</p>"},{"location":"api/samplers/#bmi.samplers._additive_uniform.AdditiveUniformSampler.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Returns a sample from the joint distribution P(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>n_points</code> <code>int</code> <p>sample size</p> required <code>rng</code> <code>Union[int, Any]</code> <p>pseudorandom number generator</p> required <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, numpy.ndarray]</code> <p>X samples, shape (n_points, dim_x) Y samples, shape (n_points, dim_y). Note that these samples are paired with X samples.</p>"},{"location":"api/samplers/#bmi.samplers._splitmultinormal.BivariateNormalSampler","title":"<code> bmi.samplers._splitmultinormal.BivariateNormalSampler            (BaseSampler)         </code>","text":"<p>A special case of a general multivariate normal sampler, where both X and Y are one-dimensional Gaussian variables with a given correlation.</p>"},{"location":"api/samplers/#bmi.samplers._splitmultinormal.BivariateNormalSampler.mutual_information","title":"<code>mutual_information(self)</code>","text":"<p>Mutual information MI(X; Y).</p>"},{"location":"api/samplers/#bmi.samplers._splitmultinormal.BivariateNormalSampler.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Returns a sample from the joint distribution P(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>n_points</code> <code>int</code> <p>sample size</p> required <code>rng</code> <code>Union[Any, int]</code> <p>pseudorandom number generator</p> required <p>Returns:</p> Type Description <code>tuple[numpy.ndarray, numpy.ndarray]</code> <p>X samples, shape (n_points, dim_x) Y samples, shape (n_points, dim_y). Note that these samples are paired with X samples.</p>"},{"location":"api/samplers/#combining-and-transforming-samplers","title":"Combining and transforming samplers","text":""},{"location":"api/samplers/#bmi.samplers._independent_coordinates.IndependentConcatenationSampler","title":"<code> bmi.samplers._independent_coordinates.IndependentConcatenationSampler            (BaseSampler)         </code>","text":"<p>Consider a sequence of samplers \\(S_k\\), where \\(k \\in \\{1, \\dotsc, m \\}\\) and variables</p> \\[(X_k, Y_k) \\sim S_k.\\] <p>If the variables are sampled independently, we can concatenate them to \\(X = (X_1, \\dotsc, X_m)\\) and \\(Y = (Y_1, \\dotsc, Y_m)\\)</p> <p>and have</p> \\[I(X; Y) = I(X_1; Y_1) + \\dotsc + I(X_m; Y_m).\\]"},{"location":"api/samplers/#bmi.samplers._independent_coordinates.IndependentConcatenationSampler.__init__","title":"<code>__init__(self, samplers)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>samplers</code> <code>Sequence[bmi.interface.ISampler]</code> <p>sequence of samplers to concatenate</p> required"},{"location":"api/samplers/#bmi.samplers._independent_coordinates.IndependentConcatenationSampler.mutual_information","title":"<code>mutual_information(self)</code>","text":"<p>Mutual information MI(X; Y).</p>"},{"location":"api/samplers/#bmi.samplers._independent_coordinates.IndependentConcatenationSampler.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Returns a sample from the joint distribution P(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>n_points</code> <code>int</code> <p>sample size</p> required <code>rng</code> <code>Union[int, Any]</code> <p>pseudorandom number generator</p> required <p>Returns:</p> Type Description <code>tuple[jax.Array, jax.Array]</code> <p>X samples, shape (n_points, dim_x) Y samples, shape (n_points, dim_y). Note that these samples are paired with X samples.</p>"},{"location":"api/samplers/#bmi.samplers._transformed.TransformedSampler","title":"<code> bmi.samplers._transformed.TransformedSampler            (BaseSampler)         </code>","text":"<p>Pushforward of a distribution \\(P_{XY}\\) via a product mapping     \\(f \\times g\\).</p> <p>In other words, we have mutual information between \\(f(X)\\) and \\(g(Y)\\) for some mappings \\(f\\) and \\(g\\).</p> <p>Note:   By default we assume that f and g are diffeomorphisms, so that       I(f(X); g(Y)) = I(X; Y).   If you don't use diffeomorphisms (in particular,   non-default <code>add_dim_x</code> or <code>add_dim_y</code>), overwrite the   <code>mutual_information()</code> method</p>"},{"location":"api/samplers/#bmi.samplers._transformed.TransformedSampler.__init__","title":"<code>__init__(self, base_sampler, transform_x=None, transform_y=None, add_dim_x=0, add_dim_y=0, vectorise=True)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>base_sampler</code> <code>ISampler</code> <p>allows sampling from \\(P(X, Y)\\)</p> required <code>transform_x</code> <code>Optional[Callable]</code> <p>diffeomorphism \\(f\\), so that we have variable \\(f(X)\\). By default the identity mapping.</p> <code>None</code> <code>transform_y</code> <code>Optional[Callable]</code> <p>diffeomorphism \\(g\\), so that we have variable \\(g(Y)\\). By default the identity mapping.</p> <code>None</code> <code>add_dim_x</code> <code>int</code> <p>the difference in dimensions of the output of \\(f\\) and its input. Note that for any diffeomorphism it must be zero</p> <code>0</code> <code>add_dim_y</code> <code>int</code> <p>similarly as <code>add_dim_x</code>, but for \\(g\\).</p> <code>0</code> <code>vectorise</code> <code>bool</code> <p>whether to use <code>jax.vmap</code> to vectorise transforms. If False, provided <code>transform_X</code> and <code>transform_Y</code> need to already be vectorized.</p> <code>True</code> <p>Note</p> <p>If you don't use diffeomorphisms (in particular, non-default <code>add_dim_x</code> or <code>add_dim_y</code>), overwrite the <code>mutual_information()</code> method</p>"},{"location":"api/samplers/#bmi.samplers._transformed.TransformedSampler.mutual_information","title":"<code>mutual_information(self)</code>","text":"<p>Mutual information MI(X; Y).</p>"},{"location":"api/samplers/#bmi.samplers._transformed.TransformedSampler.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Samples from the distribution \\(P(f(X), g(Y))\\).</p> <p>Returns:</p> Type Description <code>tuple[jax.Array, jax.Array]</code> <p>paired samples     from \\(f(X)\\), shape <code>(n_points, dim(X) + add_dim_x)</code> and     from \\(g(Y)\\), shape <code>(n_points, dim(Y) + add_dim_y)</code></p>"},{"location":"api/samplers/#bmi.samplers._transformed.TransformedSampler.transform","title":"<code>transform(self, x, y)</code>","text":"<p>Transforms given samples by <code>f x g</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[jax.Array, numpy.ndarray]</code> <p>samples, (n_points, dim(X))</p> required <code>y</code> <code>Union[jax.Array, numpy.ndarray]</code> <p>samples, (n_points, dim(Y))</p> required <p>Returns:</p> Type Description <code>tuple[jax.Array, jax.Array]</code> <p>f(x), shape (n_points, dim(X) + add_dim_x) g(y), shape (n_points, dim(Y) + add_dim_y)</p>"},{"location":"api/samplers/#discrete-random-variables","title":"Discrete random variables","text":""},{"location":"api/samplers/#bmi.samplers._discrete_continuous.DiscreteUniformMixtureSampler","title":"<code> bmi.samplers._discrete_continuous.DiscreteUniformMixtureSampler            (BaseSampler)         </code>","text":"<p>Sampler from Gao et al. (2017) for the discrete-continuous mixture model:</p> \\[X \\sim \\mathrm{Categorical}(1/m, ..., 1/m)\\] <p>is sampled from the set \\(\\{0, ..., m-1\\}\\).</p> <p>Then,</p> \\[Y | X \\sim \\mathrm{Uniform}(X, X+2).\\] <p>It holds that</p> \\[I(X; Y) = \\log m - \\frac{m-1}{m} \\log 2.\\]"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.DiscreteUniformMixtureSampler.__init__","title":"<code>__init__(self, *, n_discrete=5, use_discrete_x=True)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n_discrete</code> <code>int</code> <p>number of discrete values to sample X from</p> <code>5</code> <code>use_discrete_x</code> <code>bool</code> <p>if True, X will be an integer. If False, X will be casted to a float.</p> <code>True</code>"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.DiscreteUniformMixtureSampler.cite","title":"<code>cite()</code>  <code>staticmethod</code>","text":"<p>Returns the BibTeX citation.</p>"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.DiscreteUniformMixtureSampler.mutual_information","title":"<code>mutual_information(self)</code>","text":"<p>Mutual information MI(X; Y).</p>"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.DiscreteUniformMixtureSampler.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Returns a sample from the joint distribution P(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>n_points</code> <code>int</code> <p>sample size</p> required <code>rng</code> <code>Union[int, Any]</code> <p>pseudorandom number generator</p> required <p>Returns:</p> Type Description <code>tuple[jax.Array, jax.Array]</code> <p>X samples, shape (n_points, dim_x) Y samples, shape (n_points, dim_y). Note that these samples are paired with X samples.</p>"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.MultivariateDiscreteUniformMixtureSampler","title":"<code> bmi.samplers._discrete_continuous.MultivariateDiscreteUniformMixtureSampler            (IndependentConcatenationSampler)         </code>","text":"<p>Multivariate alternative for <code>DiscreteUniformMixtureSampler</code>, which is a concatenation of several independent samplers.</p> <p>Namely, for a sequence of integers \\((m_k)\\), the variables \\(X = (X_1, ..., X_k)\\) and \\(Y = (Y_1, ..., Y_k)\\) are sampled coordinate-wise.</p> <p>Each coordinate</p> \\[X_k \\sim \\mathrm{Categorical}(1/m_k, ..., 1/m_k)\\] <p>is from the set \\(\\{0, ..., m_k-1\\}\\).</p> <p>Then,</p> \\[Y_k | X_k \\sim \\mathrm{Uniform}(X_k, X_k + 2).\\] <p>Mutual information can be calculated as</p> \\[I(X; Y) = \\sum_k I(X_k; Y_k),\\] <p>where</p> \\[I(X_k; Y_k) = \\log m_k - \\frac{m_k-1}{m_k} \\log 2.\\]"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.MultivariateDiscreteUniformMixtureSampler.cite","title":"<code>cite()</code>  <code>staticmethod</code>","text":"<p>Returns the BibTeX citation.</p>"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.ZeroInflatedPoissonizationSampler","title":"<code> bmi.samplers._discrete_continuous.ZeroInflatedPoissonizationSampler            (BaseSampler)         </code>","text":"<p>Sampler from Gao et al. (2017) modelling zero-inflated Poissonization of the exponential distribution.</p> <p>Let \\(X \\sim \\mathrm{Exponential}(1)\\). Then, \\(Y\\) is sampled from the mixture distribution</p> \\[Y \\mid X = p\\, \\delta_0 + (1-p) \\, \\mathrm{Poisson}(X) \\]"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.ZeroInflatedPoissonizationSampler.__init__","title":"<code>__init__(self, p=0.15, use_discrete_y=True)</code>  <code>special</code>","text":"<p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>zero-inflation parameter. Must be in [0, 1).</p> <code>0.15</code>"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.ZeroInflatedPoissonizationSampler.cite","title":"<code>cite()</code>  <code>staticmethod</code>","text":"<p>Returns the BibTeX citation.</p>"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.ZeroInflatedPoissonizationSampler.mutual_information","title":"<code>mutual_information(self, truncation=None)</code>","text":"<p>Ground-truth mutual information is equal to</p> \\[I(X; Y) = (1-p) \\cdot (2 \\log 2 - \\gamma - S)\\] <p>where</p> \\[S = \\sum_{k=1}^{\\infty} \\log k \\cdot 2^{-k},\\] <p>so that the approximation</p> \\[I(X; Y) \\approx (1-p) \\cdot 0.3012 \\] <p>holds.</p> <p>Parameters:</p> Name Type Description Default <code>truncation</code> <code>Optional[int]</code> <p>if set to None, the above approximation will be used. Otherwise, the sum will be truncated at the given value.</p> <code>None</code>"},{"location":"api/samplers/#bmi.samplers._discrete_continuous.ZeroInflatedPoissonizationSampler.sample","title":"<code>sample(self, n_points, rng)</code>","text":"<p>Returns a sample from the joint distribution P(X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>n_points</code> <code>int</code> <p>sample size</p> required <code>rng</code> <code>Union[int, Any]</code> <p>pseudorandom number generator</p> required <p>Returns:</p> Type Description <code>tuple[jax.Array, jax.Array]</code> <p>X samples, shape (n_points, dim_x) Y samples, shape (n_points, dim_y). Note that these samples are paired with X samples.</p>"},{"location":"api/samplers/#bend-and-mix-models","title":"Bend and Mix Models","text":"<p>See the Bend and Mix Models subpackage API for more information.</p>"},{"location":"api/samplers/#auxiliary","title":"Auxiliary","text":""},{"location":"api/samplers/#bmi.samplers.base.BaseSampler","title":"<code> bmi.samplers.base.BaseSampler            (ISampler)         </code>","text":"<p>Partial implementation of the ISampler interface, convenient to inherit from.</p>"},{"location":"api/samplers/#bmi.samplers.base.BaseSampler.dim_x","title":"<code>dim_x: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Dimension of the space in which the <code>X</code> variable is valued.</p>"},{"location":"api/samplers/#bmi.samplers.base.BaseSampler.dim_y","title":"<code>dim_y: int</code>  <code>property</code> <code>readonly</code>","text":"<p>Dimension of the space in which the <code>Y</code> variable is valued.</p>"},{"location":"api/samplers/#bmi.samplers.base.BaseSampler.__init__","title":"<code>__init__(self, *, dim_x, dim_y)</code>  <code>special</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/tasks/","title":"Tasks","text":"<p>The following functions can be used to create tasks.</p>"},{"location":"api/tasks/#bmi.benchmark.tasks.wiggly.transform_wiggly_task","title":"<code>bmi.benchmark.tasks.wiggly.transform_wiggly_task(base_task, task_name=None)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.student.task_student_sparse","title":"<code>bmi.benchmark.tasks.student.task_student_sparse(dim_x, dim_y, df, n_interacting=2, strength=2.0, task_name=None)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.multinormal.task_multinormal_sparse","title":"<code>bmi.benchmark.tasks.multinormal.task_multinormal_sparse(dim_x, dim_y, n_interacting=2, strength=2.0, task_name=None)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.student.task_student_dense","title":"<code>bmi.benchmark.tasks.student.task_student_dense(dim_x, dim_y, df, off_diag=0.5, task_name=None)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.spiral.transform_spiral_task","title":"<code>bmi.benchmark.tasks.spiral.transform_spiral_task(base_task, speed=1.0, task_name=None, normalize_speed=True)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.rotate.transform_rotate_task","title":"<code>bmi.benchmark.tasks.rotate.transform_rotate_task(base_task, task_name=None)</code>","text":"<p>Note: we always use the same rotation.</p>"},{"location":"api/tasks/#bmi.benchmark.tasks.normal_cdf.transform_normal_cdf_task","title":"<code>bmi.benchmark.tasks.normal_cdf.transform_normal_cdf_task(base_task, task_name=None)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.multinormal.task_multinormal_dense","title":"<code>bmi.benchmark.tasks.multinormal.task_multinormal_dense(dim_x, dim_y, off_diag=0.5, task_name=None)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.bimodal_gaussians.task_bimodal_gaussians","title":"<code>bmi.benchmark.tasks.bimodal_gaussians.task_bimodal_gaussians(gaussian_correlation=0.75)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.embeddings.transform_swissroll_task","title":"<code>bmi.benchmark.tasks.embeddings.transform_swissroll_task(base_task, task_name)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.bivariate_normal.task_bivariate_normal","title":"<code>bmi.benchmark.tasks.bivariate_normal.task_bivariate_normal(gaussian_correlation=0.75)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.half_cube.transform_half_cube_task","title":"<code>bmi.benchmark.tasks.half_cube.transform_half_cube_task(base_task, task_name=None)</code>","text":""},{"location":"api/tasks/#bmi.benchmark.tasks.additive_noise.task_additive_noise","title":"<code>bmi.benchmark.tasks.additive_noise.task_additive_noise(epsilon)</code>","text":""}]}